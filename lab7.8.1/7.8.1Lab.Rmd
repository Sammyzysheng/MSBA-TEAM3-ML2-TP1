---
title: "Team 3 Presentation"
author: "Cody Clark"
date: "1/29/2020"
output: html_document
---
#Lab 7.8.1 Overview

###Objectives
1) Explore non-linear models as a more practical way of solving real-world problems
2) Gain exposure to polynomial regression and step-functions
3) Explore the `Wage` dataset and within it, the relationship between `age` and `wage`

###Dataset

The `Wage` dataset contains real wage and other data for a group of 3000 male workers in the Mid-Atlantic region in from 2003-2009. Here we will be exploring `age` as our X and `wage` as our Y.

```{R}
library(ISLR)
summary(Wage)
```

First, we fit the model using the lm() function, the poly() command just allows us to avoid a long formula with powers of age. The function returns an orthogonal polynomials, which is a linear combination of the variable.


```{R}
attach(Wage)
fit=lm(wage∼poly(age ,4) ,data=Wage)
coef(summary (fit))
```

We can also use poly() tp obtain variables directly by using raw=TRUE, and it does affects the coefficient, but not the fitted values.

```{R}
fit2=lm(wage∼poly(age ,4,raw=T),data=Wage)
coef(summary(fit2))
```

There are other ways to fit the model. one way to do is that by using the wrapper function I() to protect terms like age^2

```{R}
fit2a=lm(wage∼age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
```
We can also use cbind() function for building a matrix from a collection of vectors.

```{R}
fit2b=lm(wage∼cbind(age ,age^2,age^3, age ^4),data=Wage)
coef(fit2b)
```

Now we make a plot of that and along with standard errors. The dotted lines are the data points, while the blue line is the degree 4 polynomial fit to the Age data. We can see the points in this graph; the shape of the blue line tells us the relationship between age and Wage -  it seems to increase from beginning (20-40), leveling between 30 and 60, then decreasing after 60.

```{R}
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
```
``` {R}
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
```
Whether or not an orthogonal set of basis function is produced by the poly() function will not affect the model. The fitted values are identical.

```{R}
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit))
```

##Choosing a "Best" Model

First, we must decide what degree polynomial to use. Hypothesis testing is one way to do this. We use the anova() function to compare the five models and test for the following hypotheses:

Null: The initial, simple model (M1) is sufficient to explain the model Alternative: A more complex model (M2) is required

Note: M1’s features must be a subset of M2’s (nested models).

```{R}
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```

Comparing the p-values shows us that either a degree 3 or 4 polynomial appears to offer the best fit for the model.
The p-values can be obtained more easily as follows:

```{R}
coef(summary(fit.5))
```

The p-values are the same, and the t-statistics are the square roots of the F-statistics obtained via the anova() function.

```{R}
(-11.983)^2
```

```{R}
fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
```

# Polynomial Logistic Regression

In this instance the topic of interest is whether or not the polynomials of age can predict whether an individual earns more than $250,000 per year.

We fit a logistic regression model to a binary response variable utilzing the 'Wrapper (I)' function below. This expression (I(wage>250)) evaluates to a logical variable containing TRUEs and FALSEs, which glm() coerces into binary by setting the TRUEs to 1 and FALSEs to 0. 

```{R}
fit=glm(I(wage>250)~poly(age,4),data=Wage,family=binomial)
summary(fit)
```

Utilize the predict function to make predictions of the polynomial logistic regression model. The default prediction type for a glm() model is type="link", which means we calculate predictions for the logit (log-odds) rather than probabilities.

```{R}
preds=predict(fit,newdata=list(age=age.grid),se=T)
```

Calculating confidence intervals and standard error bands. Since the computations are logit we will need to apply inverse logit mapping.

```{R}
pfit = exp(preds$fit) / (1+exp(preds$fit))
se.bands.logit = cbind(lower=preds$fit-2*preds$se.fit,upper=preds$fit+2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))
tail(se.bands)
```

This is an example of how a type="response" prediction should be set up, however, it is yielding negative probabilities and should not be used!

```{R}
pred.p=predict(fit,newdata=list(age=age.grid),type="response",se=T)
```

Graph The Data

Below is the code and plot associated with predicted fit and standard error bands. Observed wage values greater than 250K are shown as gray marks at the top of the plot while observations of wage values less than 250K are shown as gray marks at the bottom of the plot.

```{R}
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age), I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="red",lty=3)
```

Jitter() function is used so age observations with the same value do not cover each other.
This plot is sometimes referred to as a rug plot.

Step Function

In this exercise we utilize the cut function to break the age variable into 4 segments and then build a linear model according to the segments.

```{R}
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
```

The cut() function returns an ordered categorical variable; the lm() function then creates a set of dummy variables for use in the regression. The age<33.5 category is left out, so the intercept coefficient of $94,160 can be interpreted as the average salary for those under 33.5 years of age, and the other coefficients can be interpreted as the average additional salary for those in the other age groups.
